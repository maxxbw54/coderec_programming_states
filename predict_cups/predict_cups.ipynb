{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from requests import session\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import json\n",
    "import copy\n",
    "import numpy as np, scipy.stats as st\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import logging\n",
    "import  argparse\n",
    "import math\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from itertools import chain\n",
    "import xgboost as xg\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "from sklearn import preprocessing\n",
    "import matplotlib\n",
    "from re import S\n",
    "from scipy.stats.stats import pearsonr\n",
    "from sklearn.calibration import calibration_curve\n",
    "import matplotlib\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "# sys import ..\n",
    "sys.path.append(\"../\")\n",
    "from data_split import *\n",
    "from metrics_get import *\n",
    "from collections import Counter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/featureframe_user_study.pkl'\n",
    "data = pickle.load(open(path, 'rb'))\n",
    "splitbyusers = True\n",
    "output_path = '.'\n",
    "test_percentage = 0.001\n",
    "val_percentage = 0.001\n",
    "REMOVE_S_AND_R = False # remove shown and replay\n",
    "feature_dict = {'Measurements: compCharLen, confidence, documentLength, numLines, numTokens, promptCharLen, promptEndPos, quantile': 0,\n",
    "'edit percentage': 1, 'time_in_state': 2, 'session_features':3, 'suggestion_label':4, 'prompt_label':5,\n",
    "'suggestion_embedding':6, 'prompt_embedding':7, 'suggestion_text_features':8, 'prompt_text_features':9, 'user_id':10, 'statename':11, 'labeled_state': 12}\n",
    "# split into train and test\n",
    "SEQUENCE_MODE = False # keep session as a sequence or split it into events\n",
    "SPLIT_BY_USER = bool(splitbyusers) # otherwise split by session uniformly\n",
    "ADD_PREVIOUS_STATES = True\n",
    "PREDICT_ACTION = True # Otherwise predict time in state\n",
    "NORMALIZE_DATA = False # normalize data\n",
    "\n",
    "previous_states_to_keep = 5\n",
    "if not PREDICT_ACTION and SPLIT_BY_USER:\n",
    "    raise ValueError('Cannot predict time and split by user')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all Counter({0: 276, 2: 235, 4: 172, 9: 121, 3: 68, 11: 63, 7: 40, 6: 39, 8: 39, 5: 36, 10: 5, 12: 1, 1: 1})\n",
    "#new_label_map = {0:0, 2:1,4:2,3:4,1:4,5:4,6:4,7:4,8:4,9:4,10:4,11:4,12:4}\n",
    "new_label_map = {0:0, 2:4,4:4,3:4,1:4,5:4,6:4,7:4,8:4,9:4,10:4,11:4,12:4}\n",
    "new_label_map = {0:0, 2:2,4:4,3:3,1:1,5:5,6:6,7:7,8:8,9:9,10:10,11:11,12:12}\n",
    "def transform_label(y):\n",
    "    return np.array([new_label_map[y_] for y_ in y])\n",
    "\n",
    "def normalized_counter(elements):\n",
    "    counter = Counter(elements)\n",
    "\n",
    "    # Calculate the length of the list\n",
    "    list_length = len(elements)\n",
    "\n",
    "    # Normalize frequencies by dividing by the list length\n",
    "    normalized_frequencies = {element: frequency / list_length for element, frequency in counter.items()}\n",
    "\n",
    "    return normalized_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TRIALS = 21 # number of programmers\n",
    "so_far_count = 0\n",
    "accs = []\n",
    "baseline_accs = []\n",
    "aucs = []\n",
    "baseline_all0 = []\n",
    "baseline_all4 = []\n",
    "\n",
    "test_set_seenns = []\n",
    "while so_far_count < MAX_TRIALS:\n",
    "    random_number = np.random.randint(0, 100000)\n",
    "    # without latent state\n",
    "\n",
    "    features_to_keep = np.array([0,2,3,4,5,8,9,10])\n",
    "    label_index = np.array([11])\n",
    "    df_observations_features, df_observations_labels = get_features_labels_user_study(path, features_to_keep, label_index, REMOVE_S_AND_R, False)\n",
    "\n",
    "    X_train, X_test, X_val, y_train, y_test, y_val = process_data_user_study(df_observations_features, df_observations_labels,\n",
    "    REMOVE_S_AND_R, SEQUENCE_MODE, SPLIT_BY_USER, ADD_PREVIOUS_STATES, PREDICT_ACTION, NORMALIZE_DATA,\n",
    "    test_percentage, val_percentage, previous_states_to_keep, random_number)\n",
    "\n",
    "    label_index = np.array([12])\n",
    "    df_observations_features, df_observations_labels = get_features_labels_user_study(path, features_to_keep, label_index, REMOVE_S_AND_R, True)\n",
    "\n",
    "    _, _, _, y_train, y_test, y_val = process_data_user_study(df_observations_features, df_observations_labels,\n",
    "    REMOVE_S_AND_R, SEQUENCE_MODE, SPLIT_BY_USER, ADD_PREVIOUS_STATES, PREDICT_ACTION, NORMALIZE_DATA,\n",
    "    test_percentage, val_percentage, previous_states_to_keep, random_number)\n",
    "    \n",
    "    y_test = transform_label(y_test)\n",
    "    y_train = transform_label(y_train)\n",
    "    y_val = transform_label(y_val)\n",
    "    # check if X_test array is already been seen\n",
    "    for seen in test_set_seenns:\n",
    "        if np.array_equal(X_test, seen):\n",
    "            continue\n",
    "    test_set_seenns.append(X_test)\n",
    "    so_far_count += 1\n",
    "    model = XGBClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    y_pred_proba = model.predict_proba(X_test)\n",
    "    y_pred_proba = y_pred_proba[:,1]\n",
    "    #auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    #aucs_without.append(auc)\n",
    "    accs.append(accuracy)\n",
    "    # Counter y_test with dividing by len\n",
    "    normalized_frequencies = normalized_counter(y_test)\n",
    "    print(f'Counter y_test: {normalized_frequencies}')\n",
    "    max_element = max(normalized_frequencies, key=normalized_frequencies.get)\n",
    "    max_normalized_frequency = normalized_frequencies[max_element]\n",
    "    baseline_accs.append(max_normalized_frequency)\n",
    "    baseline_all0.append(normalized_frequencies[0])\n",
    "    baseline_all4.append(normalized_frequencies[4])\n",
    "    print(f'baseline_accs: {baseline_accs}')\n",
    "    print(f'accs_without: {accs}')\n",
    "    print(f'baseline_all0: {baseline_all0}')\n",
    "    print(f'baseline_all4: {baseline_all4}')\n",
    "    #auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    #aucs.append(auc)\n",
    "    #print(f'aucs: {aucs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print mean and standard error\n",
    "print(f'xgb model: accuracy = {np.mean(accs)} +- {np.std(accs)/np.sqrt(len(accs))}')\n",
    "print(f'baseline_all0 : accuracy = {np.mean(baseline_all0)} +- {np.std(baseline_all0)/np.sqrt(len(baseline_all0))}')\n",
    "print(f'baseline_accs : accuracy = {np.mean(baseline_accs)} +- {np.std(baseline_accs)/np.sqrt(len(baseline_accs))}')\n",
    "print(f'baseline_all4 : accuracy = {np.mean(baseline_all4)} +- {np.std(baseline_all4)/np.sqrt(len(baseline_all4))}')\n",
    "print(f'xgb model: auc = {np.mean(aucs)} +- {np.std(aucs)/np.sqrt(len(aucs))}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hussein",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
